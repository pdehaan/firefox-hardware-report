{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import datetime as dt\n",
    "import os.path\n",
    "import boto3\n",
    "import botocore\n",
    "import calendar\n",
    "import requests\n",
    "import moztelemetry.standards as moz_std\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vendor_name_from_id(id):\n",
    "    \"\"\" Get the string name matching the provided vendor id.\n",
    "    \n",
    "    Args:\n",
    "        id: A string containing the vendor id.\n",
    "    \n",
    "    Returns: \n",
    "        A string containing the vendor name or \"(Other <ID>)\" if\n",
    "        unknown.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: We need to make this an external resource for easier\n",
    "    # future updates.\n",
    "    vendor_map = {\n",
    "        '0x1013': 'Cirrus Logic',\n",
    "        '0x1002': 'AMD',\n",
    "        '0x8086': 'Intel',\n",
    "        '0x5333': 'S3 Graphics',\n",
    "        '0x1039': 'SIS',\n",
    "        '0x1106': 'VIA',\n",
    "        '0x10de': 'NVIDIA',\n",
    "        '0x102b': 'Matrox',\n",
    "        '0x15ad': 'VMWare',\n",
    "        '0x80ee': 'Oracle VirtualBox',\n",
    "        '0x1414': 'Microsoft Basic',\n",
    "    }\n",
    "    \n",
    "    return vendor_map.get(id, \"Other (\" + id + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to query the longitudinal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reasons why the data for a client can be discarded.\n",
    "REASON_INACTIVE = \"inactive\"\n",
    "REASON_BROKEN_DATA = \"broken\"\n",
    "\n",
    "def get_valid_client_record(r, data_index):\n",
    "    \"\"\" Check if the referenced record is sane or contains partial/broken data.\n",
    "    \n",
    "    Args:\n",
    "        r: The client entry in the longitudinal dataset.\n",
    "        dat_index: The index of the sample within the client record.\n",
    "    \n",
    "    Returns:\n",
    "        An object containing the client hardware data or REASON_BROKEN_DATA if the\n",
    "        data is invalid.\n",
    "    \"\"\"\n",
    "    gfx_adapters = r[\"system_gfx\"][data_index][\"adapters\"]\n",
    "    monitors = r[\"system_gfx\"][data_index][\"monitors\"]\n",
    "    \n",
    "    # We should make sure to have the resolution and GFX adapter. If we don't,\n",
    "    # discard this record.\n",
    "    if not gfx_adapters or not gfx_adapters[0] or not monitors or not monitors[0]:\n",
    "        return REASON_BROKEN_DATA\n",
    "    \n",
    "    # 0x0000 seems to be an invalid vendor id. Just discard the record.\n",
    "    if gfx_adapters[0][\"vendor_id\"] == '0x0000':\n",
    "        return REASON_BROKEN_DATA\n",
    "    \n",
    "    # At this point, we should have filtered out all the weirdness. Fetch\n",
    "    # the data we need. \n",
    "    data = {\n",
    "        'os_name': r[\"system_os\"][data_index][\"name\"],\n",
    "        'os_version': r[\"system_os\"][data_index][\"version\"],\n",
    "        'memory_mb': r[\"system\"][data_index][\"memory_mb\"],\n",
    "        'gfx0_vendor_id': gfx_adapters[0][\"vendor_id\"],\n",
    "        'screen_width': monitors[0][\"screen_width\"],\n",
    "        'screen_height': monitors[0][\"screen_height\"],\n",
    "        'cpu_cores': r[\"system_cpu\"][data_index][\"cores\"],\n",
    "        'cpu_vendor': r[\"system_cpu\"][data_index][\"vendor\"]\n",
    "    }\n",
    "    \n",
    "    return REASON_BROKEN_DATA if None in data.values() else data\n",
    "\n",
    "def get_latest_valid_per_client(entry, time_start, time_end):\n",
    "    \"\"\" Get the most recently submitted ping for a client within the given timeframe.\n",
    "\n",
    "    Then use this index to look up the data from the other columns (we can assume that the sizes\n",
    "    of these arrays match, otherwise the longitudinal dataset is broken).\n",
    "    Once we have the data, we make sure it's valid and return it.\n",
    "    \n",
    "    Args:\n",
    "        entry: The record containing all the data for a single client.\n",
    "        time_start: The beginning of the reference timeframe.\n",
    "        time_end: The end of the reference timeframe.\n",
    "\n",
    "    Returns:\n",
    "        An object containing the valid hardware data for the client or a string\n",
    "        describing why the data is discarded. Either REASON_INACTIVE, if the client didn't\n",
    "        submit a ping within the desired timeframe, or REASON_BROKEN_DATA if it send\n",
    "        broken data. \n",
    "    \n",
    "    Raises:\n",
    "        ValueError: if the columns within the record have mismatching lengths. This\n",
    "        means the longitudinal dataset is corrupted.\n",
    "    \"\"\"\n",
    "    latest_entry = None\n",
    "    for index, pkt_date in enumerate(entry[\"submission_date\"]):\n",
    "        sub_date = dt.datetime.strptime(pkt_date, \"%Y-%m-%dT%H:%M:%S.%fZ\").date()\n",
    "        # The data is in descending order, the most recent ping comes first.\n",
    "        # The first item less or equal than the time_end date is our thing.\n",
    "        if sub_date >= time_start and sub_date <= time_end:\n",
    "            latest_entry = index\n",
    "            break\n",
    "        \n",
    "        # Ok, we went too far, we're not really interested in the data\n",
    "        # outside of [time_start, time_end]. Since records are ordered,\n",
    "        # we can actually skip this.\n",
    "        if sub_date < time_start:\n",
    "            break\n",
    "    \n",
    "    # This client wasn't active in the reference timeframe, just map it to no data.\n",
    "    if latest_entry is None:\n",
    "        return REASON_INACTIVE\n",
    "\n",
    "    # Some clients might be missing entire sections. Yeah, this is weird. Skip them,\n",
    "    # we don't want partial data.\n",
    "    desired_sections = [\"system_os\", \"submission_date\", \"system\", \"system_gfx\", \"system_cpu\"]\n",
    "    for field in desired_sections:\n",
    "        if entry[field] is None:\n",
    "            return REASON_BROKEN_DATA\n",
    "\n",
    "        # All arrays in the longitudinal dataset should have the same length, for a\n",
    "        # single client. If that's not the case, if our index is not there, throw.\n",
    "        if entry[field][latest_entry] is None:\n",
    "            raise ValueError(\"Null \" + field + \" index: \" + str(latest_entry))\n",
    "\n",
    "    return get_valid_client_record(entry, latest_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define how we transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(p):\n",
    "    \"\"\" This function prepares the data for further analyses (e.g. unit conversion,\n",
    "    vendor id to string, ...). \"\"\"\n",
    "    return {\n",
    "        'cpu_cores': p['cpu_cores'],\n",
    "        'cpu_vendor': p['cpu_vendor'],\n",
    "        'gfx0_vendor_name': vendor_name_from_id(p['gfx0_vendor_id']),\n",
    "        'resolution': str(p['screen_width']) + 'x' + str(p['screen_height']),\n",
    "        'memory_gb': int(round(p['memory_mb'] / 1024.0)),\n",
    "        'os': p['os_name'] + '-' + p['os_version'],\n",
    "    }\n",
    "\n",
    "def aggregate_data(processed_data):\n",
    "    def seq(acc, v):\n",
    "        # The dimensions over which we want to aggregate the different values.\n",
    "        keys_to_aggregate = [\n",
    "            'cpu_cores',\n",
    "            'cpu_vendor',\n",
    "            'gfx0_vendor_name',\n",
    "            'resolution',\n",
    "            'memory_gb',\n",
    "            'os',\n",
    "        ]\n",
    "\n",
    "        for key_name in keys_to_aggregate:\n",
    "            # We want to know how many users have a particular configuration (e.g. using a particular\n",
    "            # cpu vendor). For each dimension of interest, build a key as (hw, value) and count its\n",
    "            # occurrences among the user base.\n",
    "            acc_key = (key_name, v[key_name])\n",
    "            acc[acc_key] = acc.get(acc_key, 0) + 1\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def cmb(v1, v2):\n",
    "        # Combine the counts from the two partial dictionaries. Hacky?\n",
    "        return  { k: v1.get(k, 0) + v2.get(k, 0) for k in set(v1) | set(v2) }\n",
    "    \n",
    "    return processed_data.aggregate({}, seq, cmb)\n",
    "\n",
    "def collapse_buckets(aggregated_data, count_threshold):\n",
    "    \"\"\" Collapse uncommon configurations in generic groups to preserve privacy.\n",
    "    \n",
    "    This takes the dictionary of aggregated results from |aggregate_data| and collapses\n",
    "    entries with a value less than |count_threshold| in a generic bucket.\n",
    "    \n",
    "    Args:\n",
    "        aggregated_data: The object containing aggregated data.\n",
    "        count_threhold: Groups (or \"configurations\") containing less than this value\n",
    "        are collapsed in a generic bucket.\n",
    "    \"\"\"\n",
    "    collapsed_groups = {}\n",
    "    for k,v in aggregated_data.iteritems():\n",
    "        # Don't clump this group into the \"Other\" bucket if it has enough\n",
    "        # users it in.\n",
    "        if v > count_threshold:\n",
    "            collapsed_groups[k] = v\n",
    "            continue\n",
    "        \n",
    "        # If we're here, it means that the key has not enough elements.\n",
    "        # Fall through the next cases and try to group things together.\n",
    "        new_group_key = 'Other'\n",
    "        \n",
    "        # Let's try to group similar resolutions together.\n",
    "        key_type = k[0]\n",
    "        if key_type == 'resolution':\n",
    "            # Extract the resolution.\n",
    "            [w, h] = k[1].split('x')\n",
    "            # Round to the nearest hundred.\n",
    "            w = int(round(int(w), -2))\n",
    "            h = int(round(int(h), -2))\n",
    "            # Build up a new key.\n",
    "            new_group_key = '~' + str(w) + 'x' + str(h)\n",
    "        elif key_type == 'os':\n",
    "            [os, ver] = k[1].split('-', 1)\n",
    "            new_group_key = os + '-' + 'Other'\n",
    "        \n",
    "        # We don't have enough data for this particular group/configuration.\n",
    "        # Aggregate it with the data in the \"Other\" bucket\n",
    "        other_key = (k[0], new_group_key)\n",
    "        collapsed_groups[other_key] = collapsed_groups.get(other_key, 0) + v\n",
    "    \n",
    "    # The previous grouping might have created additional groups. Let's check again.\n",
    "    final_groups = {}\n",
    "    for k,v in collapsed_groups.iteritems():\n",
    "        # Don't clump this group into the \"Other\" bucket if it has enough\n",
    "        # users it in.\n",
    "        if v > count_threshold:\n",
    "            final_groups[k] = v\n",
    "            continue\n",
    "\n",
    "        # We don't have enough data for this particular group/configuration.\n",
    "        # Aggregate it with the data in the \"Other\" bucket\n",
    "        other_key = (k[0], 'Other')\n",
    "        final_groups[other_key] = final_groups.get(other_key, 0) + v\n",
    "    \n",
    "    return final_groups\n",
    "\n",
    "def finalize_data(data, sample_count, broken_ratio, inactive_ratio, report_date):\n",
    "    \"\"\" Finalize the aggregated data.\n",
    "    \n",
    "    Translate raw sample numbers to percentages and add the date for the reported\n",
    "    week along with the percentage of discarded samples due to broken data.\n",
    "    \n",
    "    Rename the keys to more human friendly names.\n",
    "    \n",
    "    Args:\n",
    "        data: Data in aggregated form.\n",
    "        sample_count: The number of samples the aggregates where generated from.\n",
    "        broken_ratio: The percentage of samples discarded due to broken data.\n",
    "        inactive_ratio: The percentage of samples discarded due to the client not sending data.\n",
    "        report_date: The starting day for the reported week.\n",
    "    \n",
    "    Returns:\n",
    "        An object containing the reported hardware statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    denom = float(sample_count)\n",
    "\n",
    "    aggregated_percentages = {\n",
    "        'date': report_date.isoformat(),\n",
    "        'broken': broken_ratio,\n",
    "        'inactive': inactive_ratio,\n",
    "    }\n",
    "\n",
    "    keys_translation = {\n",
    "        'cpu_cores': 'cores_',\n",
    "        'cpu_vendor': 'cpu_',\n",
    "        'gfx0_vendor_name': 'gpu_',\n",
    "        'resolution': 'display_',\n",
    "        'memory_gb': 'ram_',\n",
    "        'os': 'os_',\n",
    "    }\n",
    "\n",
    "    # Compute the percentages from the raw numbers.\n",
    "    for k, v in data.iteritems():\n",
    "        # The old key is a tuple (key, value). We translate the key part and concatenate the\n",
    "        # value as a string.\n",
    "        new_key = keys_translation[k[0]] + unicode(k[1])\n",
    "        aggregated_percentages[new_key] = v / denom\n",
    "\n",
    "    return aggregated_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File and S3 serialization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S3_PUBLIC_BUCKET = \"telemetry-public-analysis-2\"\n",
    "S3_DATA_PATH = \"game-hardware-survey/data/\"\n",
    "\n",
    "def get_file_name(suffix=\"\"):\n",
    "    return \"hwsurvey-weekly\" + suffix + \".json\"\n",
    "\n",
    "def serialize_results(aggregated_data, week_start, week_end):\n",
    "    # Write the week start/end in the filename.\n",
    "    suffix = \"-\" + week_start.strftime(\"%Y%d%m\") + \"-\" + week_end.strftime(\"%Y%d%m\")\n",
    "    file_name = get_file_name(suffix)\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print \"{} exists, we will overwrite it.\".format(file_name)\n",
    "\n",
    "    # Our aggregated data is a JSON object.\n",
    "    json_entry = json.dumps(aggregated_data)\n",
    "\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(\"[\" + json_entry.encode('utf8') + \"]\\n\")\n",
    "\n",
    "def fetch_previous_state(s3_source_file_name, local_file_name):\n",
    "    \"\"\"\n",
    "    This function fetches the previous state from S3's bucket and stores it locally.\n",
    "    \n",
    "    Args:\n",
    "        s3_source_file_name: The name of the file on S3.\n",
    "        local_file_name: The name of the file to save to, locally.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the previous state.\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "    key_path = S3_DATA_PATH + s3_source_file_name\n",
    "    \n",
    "    try:\n",
    "        transfer.download_file(S3_PUBLIC_BUCKET, key_path, local_file_name)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # If the file wasn't there, that's ok. Otherwise, abort!\n",
    "        if e.response['Error']['Code'] != \"404\":\n",
    "            raise e\n",
    "        else:\n",
    "            print \"Did not find an existing file at '{}'\".format(key_path)\n",
    "\n",
    "def store_new_state(source_file_name, s3_dest_file_name):\n",
    "    \"\"\"\n",
    "    Store the new state file to S3.\n",
    "    \n",
    "    Args:\n",
    "        source_file_name: The name of the local source file.\n",
    "        s3_dest_file_name: The name of the destination file on S3.\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "    \n",
    "    # Update the state in the analysis bucket.\n",
    "    key_path = S3_DATA_PATH + s3_dest_file_name\n",
    "    transfer.upload_file(source_file_name, S3_PUBLIC_BUCKET, key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main logic, wiring all the things together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_report(start_date=None, end_date=None):\n",
    "    \"\"\" Generates the hardware survey dataset for the reference timeframe.\n",
    "    \n",
    "    If the timeframe is longer than a week, split it in in weekly chunks\n",
    "    and process each chunk individually (eases backfilling).\n",
    "    \n",
    "    The report for each week is saved in a local JSON file.\n",
    "    \n",
    "    Args:\n",
    "        start_date: The date from which we start generating the report. If None,\n",
    "           the report starts from the beginning of the past week (Sunday).\n",
    "        end_date: The date the marks the end of the reporting period. This only\n",
    "           makes sense if a |start_date| was provided. If None, this defaults\n",
    "           to the end of the past week (Saturday).\n",
    "    \"\"\"\n",
    "\n",
    "    # If no start_date was provided, generate a report for the past complete week.\n",
    "    last_week = moz_std.get_last_week_range()\n",
    "    date_range = (\n",
    "        moz_std.snap_to_beginning_of_week(start_date, \"Sunday\") if start_date != None else last_week[0],\n",
    "        end_date if (end_date != None and start_date != None) else last_week[1]\n",
    "    )\n",
    "\n",
    "    # Connect to the longitudinal dataset.\n",
    "    sqlQuery = \"SELECT \" +\\\n",
    "               \"build,\" +\\\n",
    "               \"client_id,\" +\\\n",
    "               \"system_os,\" +\\\n",
    "               \"submission_date,\" +\\\n",
    "               \"system,\" +\\\n",
    "               \"geo_country,\" +\\\n",
    "               \"system_gfx,\" +\\\n",
    "               \"system_cpu, \" +\\\n",
    "               \"normalized_channel \" +\\\n",
    "               \"FROM longitudinal\"\n",
    "    frame = sqlContext.sql(sqlQuery)\\\n",
    "                      .where(\"normalized_channel = 'release'\")\\\n",
    "                      .where(\"build is not null and build[0].application_name = 'Firefox'\")\n",
    "\n",
    "        \n",
    "    # The number of all the fetched records (including inactive and broken).\n",
    "    records_count = frame.count()\n",
    "\n",
    "    # Split the submission period in chunks, so we don't run out of resources while aggregating if\n",
    "    # we want to backfill.\n",
    "    chunk_start = date_range[0]\n",
    "    chunk_end = None\n",
    "\n",
    "    while chunk_start < date_range[1]:\n",
    "        chunk_end = chunk_start + dt.timedelta(days=6)\n",
    "\n",
    "        # Fetch the data we need.\n",
    "        data = frame.rdd.map(lambda r: get_latest_valid_per_client(r, chunk_start, chunk_end))\n",
    "        \n",
    "        # Filter out broken data.\n",
    "        filtered_data = data.filter(lambda r: r not in [REASON_BROKEN_DATA, REASON_INACTIVE])\n",
    "        \n",
    "        # Count the broken records.\n",
    "        broken_count = data.filter(lambda r: r is REASON_BROKEN_DATA).count()\n",
    "        \n",
    "        # Count and log the inactive clients.\n",
    "        inactive_count = data.filter(lambda r: r is REASON_INACTIVE).count()\n",
    "        \n",
    "        # Process the data, transforming it in the form we desire.\n",
    "        processed_data = filtered_data.map(prepare_data)\n",
    "        \n",
    "        print \"Aggregating entries...\"\n",
    "        aggregated_pings = aggregate_data(processed_data)\n",
    "\n",
    "        # Get the sample count, we need it to compute the percentages instead of raw numbers.\n",
    "        # Since we're getting only the newest ping for each client, we can simply count the\n",
    "        # number of pings. THIS MAY NOT BE CONSTANT ACROSS WEEKS!\n",
    "        valid_records_count = filtered_data.count()\n",
    "\n",
    "        # Collapse together groups that count less than 1% of our samples.\n",
    "        threshold_to_collapse = int(valid_records_count * 0.01)\n",
    "        \n",
    "        print \"Collapsing smaller groups into the other bucket (threshold {th})\".format(th=threshold_to_collapse)\n",
    "        collapsed_aggregates = collapse_buckets(aggregated_pings, threshold_to_collapse)\n",
    "        \n",
    "        print \"Post-processing raw values...\"\n",
    "        broken_ratio = broken_count / float(records_count)\n",
    "        inactive_ratio = inactive_count / float(records_count)\n",
    "        \n",
    "        processed_aggregates = finalize_data(collapsed_aggregates,\n",
    "                                             valid_records_count,\n",
    "                                             broken_ratio,\n",
    "                                             inactive_ratio,\n",
    "                                             chunk_start)\n",
    "        \n",
    "        print \"Serializing results locally...\"\n",
    "        # This either appends to an existing file, or creates a new one.\n",
    "        serialize_results(processed_aggregates, chunk_start, chunk_end)\n",
    "\n",
    "        # Move on to the next chunk, just add one day the end of the last chunk.\n",
    "        chunk_start = chunk_end + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = None # Only use this when backfilling, e.g. dt.date(2016,2,1)\n",
    "end_date = None # Only use this when backfilling, e.g. dt.date(2016,3,26)\n",
    "\n",
    "# Fetch the previous data from S3 and save it locally.\n",
    "fetch_previous_state(\"hwsurvey-weekly.json\", \"hwsurvey-weekly-prev.json\")\n",
    "# Generate the report for the desired period.\n",
    "generate_report(start_date, end_date)\n",
    "# Concat the json files into the output.\n",
    "print \"Joining JSON files...\"\n",
    "!jq -s \"[.[]|.[]]\" *.json > \"hwsurvey-weekly.json\"\n",
    "# Store the new state to S3. Since S3 doesn't support symlinks, make two copy\n",
    "# of the file: one will always contain the latest data, the other for archiving.\n",
    "archived_file_copy = \"hwsurvey-weekly-\" + datetime.date.today().strftime(\"%Y%d%m\") + \".json\"\n",
    "store_new_state(\"hwsurvey-weekly.json\", archived_file_copy)\n",
    "store_new_state(\"hwsurvey-weekly.json\", \"hwsurvey-weekly.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
